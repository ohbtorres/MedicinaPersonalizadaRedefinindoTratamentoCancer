{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivo com as funções bases para a modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para ler os arquivos variant disponibilizados e convertendo em dataframe\n",
    "def convert_variant_df(read):\n",
    "    lista = open(read, \"r\",encoding=\"utf8\").readlines()\n",
    "    #Esta lista possui \\n junto ao texto, entao vamos remover\n",
    "    lista_nova = [texto.split(sep=\"\\n\")[0].split(\",\") for texto in lista]\n",
    "    df = pd.DataFrame(lista_nova[1:],columns=lista_nova[0])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para converter os demais arquivos em dataframe\n",
    "def convert__df(read):\n",
    "    #separa o texto pelo delimitador ||\n",
    "    lista = re.split('([0-9]+)(\\|\\|)',open(read, \"r\",encoding=\"utf8\").read())\n",
    "    #Remove da lista os elementos ||\n",
    "    lista = [elemento for elemento in lista if elemento != \"||\"]\n",
    "    #Detecta o titulo do df\n",
    "    titulo = lista[0].split(\"\\n\")[0].split(\",\")\n",
    "    lista_nova= [[lista[index+1],lista[index+2]] for index in range(0,len(lista[1:])-1,2)]\n",
    "    \n",
    "    df = pd.DataFrame(lista_nova,columns=titulo)\n",
    "    return(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função que retorna um dataframe de descrição de dados (tal qual a função describe do pacote explore do R)\n",
    "def explore_describe(df):\n",
    "    df_out = pd.DataFrame(columns = ['variable','type','na' ,'na_pct' ,'unique','min', 'quat25','median','mean', \\\n",
    "                                     'quat75','max','std','skewness','kurtosis','media_desvio'])\n",
    "    df_out['variable'] = df.columns\n",
    "    df_out['type'] = df.dtypes.values\n",
    "    df_out['na'] = [sum(df[coluna].isna()) for coluna in df.columns]\n",
    "    df_out['na_pct'] = [str(round(100*sum(df[coluna].isna())/df.shape[0],1))+'%' for coluna in df.columns]\n",
    "    df_out['unique'] = [len(df[coluna].unique()) for coluna in df.columns]\n",
    "    df_out['min']  = [round(min(df[coluna]),2) if 'int' in str(df[coluna].dtype) or 'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['mean'] = [round(df[coluna].mean(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['max']  = [round(max(df[coluna]),2) if 'int' in str(df[coluna].dtype) or 'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['std'] = [round(df[coluna].std(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['quat25'] = [round(df[coluna].quantile(0.25),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['quat75'] = [round(df[coluna].quantile(0.75),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['median'] = [round(df[coluna].quantile(0.5),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['skewness'] = [round(df[coluna].skew(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                          'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['kurtosis'] = [round(df[coluna].kurt(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                          'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    \n",
    "    df_out_media_desvio_list = []\n",
    "    for coluna in df.columns:\n",
    "        if(('int' in str(df[coluna].dtype)) or ('float' in str(df[coluna].dtype)) ):\n",
    "            if((all(df[coluna] == 0)) or (df[coluna].std() == 0)):\n",
    "                df_out_media_desvio_list.append(0)\n",
    "            else:\n",
    "                df_out_media_desvio_list.append(round(df[coluna].mean()/df[coluna].std(),2))\n",
    "        else:\n",
    "            df_out_media_desvio_list.append('-')\n",
    "    \n",
    "    df_out['media_desvio'] = df_out_media_desvio_list\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para remover caracteres não ASCII\n",
    "def removeNoAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para gerar corpus (lista de documentos)\n",
    "def corpusnization(text):\n",
    "    #Removendo a pontuação e tokenizando\n",
    "    nopunct_token = nltk.tokenize.regexp_tokenize(text.lower(),\"[\\w']+\")\n",
    "   \n",
    "    #Removendo stopwords\n",
    "    token_no_stopwords = [word for word in nopunct_token if word not in stopwords.words('english')]\n",
    "    \n",
    "    #Stemming\n",
    "    #cooking -> cook\n",
    "    token_stem = [PorterStemmer().stem(token) for token in token_no_stopwords]\n",
    "    \n",
    "    #Lemmatization\n",
    "    #mice -> mouse\n",
    "    token_final = [WordNetLemmatizer().lemmatize(token) for token in token_stem]\n",
    "    return(token_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para criar o set de palavras (em ordem alfabética e sem repetição)\n",
    "def cria_listagem_palavras(corpus):\n",
    "    listagem = set()\n",
    "    for i in corpus:\n",
    "        listagem = listagem.union(set(i))\n",
    "    return(listagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta função irá retornar uma estrutra para o dicionario de palavras, atribuindo indices a elas\n",
    "def cria_dicionario_palavras(listagem):\n",
    "    dicionario = {i:j for j,i in enumerate(listagem)}\n",
    "    return(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos contar a quantidade de palavras em cada elemento do corpus\n",
    "def conta_palavras_corpus(elem_corpus):\n",
    "    return dict(Counter(elem_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para criar o dicionario de palavras\n",
    "def df_dict(listagem,bag_words_corpus):\n",
    "    return {token: sum([token in doc.keys() for doc in bag_words_corpus]) for token in listagem}\n",
    "    #total_palavras = [{i:0 for i in listagem}]\n",
    "    #total_palavras.extend(bag_words_corpus)\n",
    "    #return(dict(reduce(lambda x,y: Counter(x) + Counter(y), total_palavras)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf(termo,documento) = contagem de termo em documento / número de palavras em documento\n",
    "#idf (termo) = log (N / (df + 1))\n",
    "\n",
    "#tf-idf (termo, documento) = tf (termo, documento) * log (N / (df + 1))\n",
    "def calcula_tf_e_idf(bow, df,N):\n",
    "    #Calculando a frequencia do termo para cada documento\n",
    "    tf = [{key:t/sum(documento.values()) for key,t in documento.items()} for documento in bow]\n",
    "    idf = {chave: np.log(N/(valor+1))  for chave,valor in df.items()}\n",
    "    return (tf,idf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(lista_tf,dict_idf):\n",
    "    return [{chave: tf*dict_idf[chave] for chave,tf in doc.items()} for doc in lista_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar nuvem de palavras\n",
    "def word_cloud_plot(classe,df,bag_words):\n",
    "    indices = df[df[\"Class\"] == classe].index\n",
    "    bag_words_filtrada = [bag_words[i] for i in indices]\n",
    "    dicionario_classe = reduce(lambda x,y: Counter(x) + Counter(y), bag_words_filtrada)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate_from_frequencies(dicionario_classe)\n",
    "    return(wordcloud)\n",
    "    #plt.imshow(wordcloud)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    #return({k: v for k, v in sorted(dicionario_classe.items(), key=lambda item: item[1],reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que cria uma nuvem de palvras baseado não na frequencia, mas sim, no TF IDF\n",
    "def word_cloud_plot_tfidf(classe,df,lista_tfidf):\n",
    "    indices = df[df[\"Class\"] == classe].index\n",
    "    lista_tfidf_filtrada = [lista_tfidf[i] for i in indices]\n",
    "    dicionario_classe_tf_idf = reduce(lambda x, y: dict((chave, valor + y[chave]) if chave in y.keys() else (chave, valor) for chave, valor in x.items()), tfidf)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate_from_frequencies(dicionario_classe_tf_idf)\n",
    "    return(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para plotar a nuvem e palavras\n",
    "def plota_wordcloud(func,df,lista_dicionario_palavras):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for i in range(1,10):\n",
    "        ax = fig.add_subplot(3,3,i)\n",
    "        ax.imshow(func(i,df_training,bag_words_corpus))\n",
    "        ax.set_title(str(i))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para criar o modelo base, precisamos de uma base de dados inicial. Para isso, vamos criar o dataset usando as palavras da listagem\n",
    "def matriz_esparca(dicionario,tfidf_,N):\n",
    "    S = sp.dok_matrix((N,len(dicionario)),dtype = np.float32)\n",
    "    for i,doc in enumerate(tfidf_):\n",
    "        for chave,valor in doc.items():\n",
    "            S[i,dicionario[chave]] = valor\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para converter a matriz esparça em tfSparce\n",
    "def convert_matriz_esparca_tfSparce(M):\n",
    "    coo = M.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return(tf.SparseTensor(indices, coo.data, coo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
