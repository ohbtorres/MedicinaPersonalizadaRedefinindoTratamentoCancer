{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquivo com as funções bases para a modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential,Input,Model, load_model\n",
    "from keras.metrics import AUC\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model,to_categorical\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import string\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import json\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para ler os arquivos variant disponibilizados e convertendo em dataframe\n",
    "def convert_variant_df(read):\n",
    "    lista = open(read, \"r\",encoding=\"utf8\").readlines()\n",
    "    #Esta lista possui \\n junto ao texto, entao vamos remover\n",
    "    lista_nova = [texto.split(sep=\"\\n\")[0].split(\",\") for texto in lista]\n",
    "    df = pd.DataFrame(lista_nova[1:],columns=lista_nova[0])\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para converter os demais arquivos em dataframe\n",
    "def convert__df(read):\n",
    "    #separa o texto pelo delimitador ||\n",
    "    lista = re.split('([0-9]+)(\\|\\|)',open(read, \"r\",encoding=\"utf8\").read())\n",
    "    #Remove da lista os elementos ||\n",
    "    lista = [elemento for elemento in lista if elemento != \"||\"]\n",
    "    #Detecta o titulo do df\n",
    "    titulo = lista[0].split(\"\\n\")[0].split(\",\")\n",
    "    lista_nova= [[lista[index+1],lista[index+2]] for index in range(0,len(lista[1:])-1,2)]\n",
    "    \n",
    "    df = pd.DataFrame(lista_nova,columns=titulo)\n",
    "    return(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma função que retorna um dataframe de descrição de dados (tal qual a função describe do pacote explore do R)\n",
    "def explore_describe(df):\n",
    "    df_out = pd.DataFrame(columns = ['variable','type','na' ,'na_pct' ,'unique','min', 'quat25','median','mean', \\\n",
    "                                     'quat75','max','std','skewness','kurtosis','media_desvio'])\n",
    "    df_out['variable'] = df.columns\n",
    "    df_out['type'] = df.dtypes.values\n",
    "    df_out['na'] = [sum(df[coluna].isna()) for coluna in df.columns]\n",
    "    df_out['na_pct'] = [str(round(100*sum(df[coluna].isna())/df.shape[0],1))+'%' for coluna in df.columns]\n",
    "    df_out['unique'] = [len(df[coluna].unique()) for coluna in df.columns]\n",
    "    df_out['min']  = [round(min(df[coluna]),2) if 'int' in str(df[coluna].dtype) or 'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['mean'] = [round(df[coluna].mean(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['max']  = [round(max(df[coluna]),2) if 'int' in str(df[coluna].dtype) or 'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['std'] = [round(df[coluna].std(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['quat25'] = [round(df[coluna].quantile(0.25),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['quat75'] = [round(df[coluna].quantile(0.75),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['median'] = [round(df[coluna].quantile(0.5),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                      'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['skewness'] = [round(df[coluna].skew(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                          'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    df_out['kurtosis'] = [round(df[coluna].kurt(),2) if 'int' in str(df[coluna].dtype) or \\\n",
    "                          'float' in str(df[coluna].dtype) else '-' for coluna in df.columns]\n",
    "    \n",
    "    df_out_media_desvio_list = []\n",
    "    for coluna in df.columns:\n",
    "        if(('int' in str(df[coluna].dtype)) or ('float' in str(df[coluna].dtype)) ):\n",
    "            if((all(df[coluna] == 0)) or (df[coluna].std() == 0)):\n",
    "                df_out_media_desvio_list.append(0)\n",
    "            else:\n",
    "                df_out_media_desvio_list.append(round(df[coluna].mean()/df[coluna].std(),2))\n",
    "        else:\n",
    "            df_out_media_desvio_list.append('-')\n",
    "    \n",
    "    df_out['media_desvio'] = df_out_media_desvio_list\n",
    "    return(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para remover caracteres não ASCII\n",
    "def removeNoAscii(s):\n",
    "    return \"\".join(i for i in s if ord(i) < 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para gerar corpus (lista de documentos)\n",
    "def corpusnization(text):\n",
    "    #Removendo a pontuação e tokenizando\n",
    "    nopunct_token = nltk.tokenize.regexp_tokenize(text.lower(),\"[\\w']+\")\n",
    "   \n",
    "    #Removendo stopwords\n",
    "    token_no_stopwords = [word for word in nopunct_token if word not in stopwords.words('english')]\n",
    "    \n",
    "    #Stemming\n",
    "    #cooking -> cook\n",
    "    token_stem = [PorterStemmer().stem(token) for token in token_no_stopwords]\n",
    "    \n",
    "    #Lemmatization\n",
    "    #mice -> mouse\n",
    "    token_final = [WordNetLemmatizer().lemmatize(token) for token in token_stem]\n",
    "    return(token_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#função para criar o set de palavras (em ordem alfabética e sem repetição)\n",
    "def cria_listagem_palavras(corpus):\n",
    "    listagem = set()\n",
    "    for i in corpus:\n",
    "        listagem = listagem.union(set(i))\n",
    "    return(listagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esta função irá retornar uma estrutra para o dicionario de palavras, atribuindo indices a elas\n",
    "def cria_dicionario_palavras(listagem):\n",
    "    dicionario = {i:j for j,i in enumerate(listagem)}\n",
    "    return(dicionario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos contar a quantidade de palavras em cada elemento do corpus\n",
    "def conta_palavras_corpus(elem_corpus):\n",
    "    return dict(Counter(elem_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para criar o dicionario de palavras\n",
    "def df_dict(listagem,bag_words_corpus):\n",
    "    return {token: sum([token in doc.keys() for doc in bag_words_corpus]) for token in listagem}\n",
    "    #total_palavras = [{i:0 for i in listagem}]\n",
    "    #total_palavras.extend(bag_words_corpus)\n",
    "    #return(dict(reduce(lambda x,y: Counter(x) + Counter(y), total_palavras)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf(termo,documento) = contagem de termo em documento / número de palavras em documento\n",
    "#idf (termo) = log (N / (df + 1))\n",
    "\n",
    "#tf-idf (termo, documento) = tf (termo, documento) * log (N / (df + 1))\n",
    "def calcula_tf_e_idf(bow, df,N):\n",
    "    #Calculando a frequencia do termo para cada documento\n",
    "    tf = [{key:t/sum(documento.values()) for key,t in documento.items()} for documento in bow]\n",
    "    idf = {chave: np.log(N/(valor+1))  for chave,valor in df.items()}\n",
    "    return (tf,idf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(lista_tf,dict_idf):\n",
    "    return [{chave: tf*dict_idf[chave] for chave,tf in doc.items()} for doc in lista_tf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para criar nuvem de palavras\n",
    "def word_cloud_plot(classe,df,bag_words):\n",
    "    indices = df[df[\"Class\"] == classe].index\n",
    "    bag_words_filtrada = [bag_words[i] for i in indices]\n",
    "    dicionario_classe = reduce(lambda x,y: Counter(x) + Counter(y), bag_words_filtrada)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate_from_frequencies(dicionario_classe)\n",
    "    return(wordcloud)\n",
    "    #plt.imshow(wordcloud)\n",
    "    #plt.axis(\"off\")\n",
    "    #plt.show()\n",
    "    #return({k: v for k, v in sorted(dicionario_classe.items(), key=lambda item: item[1],reverse=True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função que cria uma nuvem de palvras baseado não na frequencia, mas sim, no TF IDF\n",
    "def word_cloud_plot_tfidf(classe,df,lista_tfidf):\n",
    "    indices = df[df[\"Class\"] == classe].index\n",
    "    lista_tfidf_filtrada = [lista_tfidf[i] for i in indices]\n",
    "    dicionario_classe_tf_idf = reduce(lambda x, y: dict((chave, valor + y[chave]) if chave in y.keys() else (chave, valor) for chave, valor in x.items()), tfidf)\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                background_color ='white', \n",
    "                min_font_size = 10).generate_from_frequencies(dicionario_classe_tf_idf)\n",
    "    return(wordcloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para plotar a nuvem e palavras\n",
    "def plota_wordcloud(func,df,lista_dicionario_palavras):\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    for i in range(1,10):\n",
    "        ax = fig.add_subplot(3,3,i)\n",
    "        ax.imshow(func(i,df_training,bag_words_corpus))\n",
    "        ax.set_title(str(i))\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para criar o modelo base, precisamos de uma base de dados inicial. Para isso, vamos criar o dataset usando as palavras da listagem\n",
    "def matriz_esparca(d,tfidf_,N_):\n",
    "    S = sp.dok_matrix((N_,len(d)),dtype = np.float32)\n",
    "    for i,doc in enumerate(tfidf_):\n",
    "        for chave,valor in doc.items():\n",
    "            S[i,d[chave]] = valor\n",
    "    \n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para converter a matriz esparça em tfSparce\n",
    "def convert_matriz_esparca_tfSparce(M):\n",
    "    coo = M.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return(tf.SparseTensor(indices, coo.data, coo.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para criar sempre os mesmos arquivos de treino e validação, para não termos diferenças nos treinamentos\n",
    "#Necessária para continuar os estudos no outro dia\n",
    "def train_valid_split(df_treino):\n",
    "    #Dividindo os dados de treino e teste, para verificar o quao bom nosso modelo está\n",
    "    x_train, x_test, y_train, y_test = train_test_split(df_treino.drop(columns = \"Class\"), df_treino[\"Class\"], \n",
    "                                                        test_size=0.3, random_state=42)\n",
    "    x_train,x_valid,y_train,y_valid =  train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return (x_train,x_valid,y_train,y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variáveis de entrada\n",
    "def dados_modelo_treino_valid(x_train,x_valid,df_training,dicionario,tfidf,N):\n",
    "    X = matriz_esparca(dicionario,tfidf,N)\n",
    "    \n",
    "    X_train = convert_matriz_esparca_tfSparce(X[x_train.index,:])\n",
    "    X_valid = convert_matriz_esparca_tfSparce(X[x_valid.index,:])\n",
    "\n",
    "    #Classes (vamos colocar as classes de 0 a 8)\n",
    "    Y = df_training.Class.apply(lambda x: x-1).to_numpy()\n",
    "    Y_train = to_categorical(Y[x_train.index])\n",
    "    Y_valid = to_categorical(Y[x_valid.index])\n",
    "    \n",
    "    return(X,X_train,X_valid,Y_train,Y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
